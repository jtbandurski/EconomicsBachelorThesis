install_tensorflow()
library(tensorflow)
install_tensorflow()
Y
install.packages("keras")
knitr::opts_chunk$set(echo = TRUE)
library(data.table)
library(tensorflow)
library(keras)
library(ggplot2)
library(ggfortify)
library(gridExtra)
library(tidyverse)
library(tidyr)
library(purrr)
# Wczytywanie danych:
# wywołalibyśmy
cifar <- dataset_cifar10()
# Wczytywanie danych:
# wywołalibyśmy
#cifar <- dataset_cifar10()
# lecz dla oszczędzenia czasu...
load("cifar.RData")
knitr::opts_chunk$set(echo = TRUE)
library(data.table)
library(tensorflow)
library(keras)
library(ggplot2)
library(ggfortify)
library(gridExtra)
library(tidyverse)
library(tidyr)
library(purrr)
# Wczytywanie danych:
# wywołalibyśmy
#cifar <- dataset_cifar10()
# lecz dla oszczędzenia czasu...
load("cifar.RData")
# wyodrębmy zbiór treningowy oraz testowy
train_images <- cifar$train$x
train_labels <- cifar$train$y
dim(train_images)
dim(train_labels)
# alternatywnie, powyższą operację możemy tez zapisać jako:
c(test_images, test_labels) %<-% cifar$test
dim(test_images)
dim(test_images)
class_names <- c('airplane', 'automobile', 'bird', 'cat', 'deer',
'dog', 'frog', 'horse', 'ship', 'truck')
# możemy sprawdzić, czy etykiety mają sens
idx <- 1:20
par(mfcol = c(4,5), mar = rep(1, 4), oma = rep(0.2, 4))
train_images[idx,,,] %>%
purrr::array_tree(1) %>%
purrr::set_names(class_names[train_labels[idx] + 1]) %>%
purrr::map(as.raster, max = 255) %>%
purrr::iwalk(~{plot(.x); title(.y)})
# Zaczniemy od przeskalowania tensorów z bitmapami, tak by zawierały wartości z przedziału [0, 1]
train_images <- train_images / 255
test_images <- test_images / 255
# teraz możemy zdefiniować model wykorzystując Keras Sequential API
dense_model <- keras_model_sequential()
# pierwsza warstwa 'spłaszcza' obrazki, dalej mamy dwie warstwy ukryte z aktywacją 'relu' i warstwę wyjściową
dense_model %>%
layer_flatten(input_shape = c(32, 32, 3)) %>%
layer_dense(units = 256, activation = 'relu') %>%
layer_dense(units = 128, activation = 'relu') %>%
layer_dense(units = 10, activation = 'softmax')
summary(dense_model)
# po zdefiniowaniu modelu musimy go zainicjować
dense_model %>% compile(
optimizer = 'adam',
loss = 'sparse_categorical_crossentropy',
metrics = c('accuracy')
)
# zdefiniowany model należy wytrenować
dense_model %>% fit(train_images, train_labels,
epochs = 10,
validation_data = list(test_images, test_labels),
verbose = 2)
# możemy sprawdzić jak dobrze radzi sobie model na zbiorze testowym
dense_score <- evaluate(dense_model, test_images, test_labels, verbose = 0)
cat('Test loss:', dense_score["loss"], "\n")
cat('Test accuracy:', dense_score["accuracy"], "\n")
# predykcje modelu możemy uzyskać wykorzystując metodę predict
predictions <- predict(dense_model, test_images)
head(predictions)
which.max(predictions[1, ]) - 1
# możemy też od razu uzyskać przewidziane klasy
class_pred <- predict_classes(dense_model, test_images)
class_pred[1]
# zdefiniujmy teraz sieć konwolucyjną wykorzystuąc częsty wzorzec - naprzemiennie ułożone warstwy konwolucyjne oraz 'pooling'
conv_model <- keras_model_sequential()
conv_model %>%
layer_conv_2d(filters = 32, kernel_size = c(3,3), activation = "relu",
input_shape = c(32,32,3)) %>%
layer_max_pooling_2d(pool_size = c(2,2)) %>%
layer_conv_2d(filters = 64, kernel_size = c(3,3), activation = "relu") %>%
layer_max_pooling_2d(pool_size = c(2,2)) %>%
layer_conv_2d(filters = 64, kernel_size = c(3,3), activation = "relu")
summary(conv_model)
# następnie, dodajmy jedną gęstą warstwę ukrytą i warstwę wyjściową
conv_model %>%
layer_flatten() %>%
layer_dense(units = 64, activation = "relu") %>%
layer_dense(units = 10, activation = "softmax")
summary(conv_model)
# inicjalizacja modelu - tak samo jak poprzednio
conv_model %>% compile(
optimizer = "adam",
loss = "sparse_categorical_crossentropy",
metrics = "accuracy"
)
# Możemy zapisać i zwizualizować przebieg procesu trenowania
training_process <-  fit(conv_model,
x = train_images, y = train_labels,
epochs = 10,
validation_data = list(test_images, test_labels),
verbose = 2)
plot(training_process)
# ewaluacja na zbiorze testowym
conv_score <- evaluate(conv_model, test_images, test_labels, verbose = 0)
cat('Test loss:', conv_score["loss"], "\n")
cat('Test accuracy:', conv_score["accuracy"], "\n")
# zdefiniujmy teraz sieć konwolucyjną wykorzystuąc częsty wzorzec - naprzemiennie ułożone warstwy konwolucyjne oraz 'pooling'
conv_model <- keras_model_sequential()
conv_model %>%
layer_batch_normalization( input_shape = c(32,32,3)) %>%
layer_conv_2d(filters = 32, kernel_size = c(3,3), activation = "relu") %>%
layer_max_pooling_2d(pool_size = c(2,2)) %>%
layer_conv_2d(filters = 64, kernel_size = c(3,3), activation = "relu") %>%
layer_max_pooling_2d(pool_size = c(2,2)) %>%
layer_conv_2d(filters = 64, kernel_size = c(3,3), activation = "relu")
summary(conv_model)
# następnie, dodajmy jedną gęstą warstwę ukrytą i warstwę wyjściową
conv_model %>%
layer_flatten() %>%
layer_dense(units = 64, activation = "relu") %>%
layer_dense(units = 10, activation = "softmax")
summary(conv_model)
# inicjalizacja modelu - tak samo jak poprzednio
conv_model %>% compile(
optimizer = "adam",
loss = "sparse_categorical_crossentropy",
metrics = "accuracy"
)
# Możemy zapisać i zwizualizować przebieg procesu trenowania
training_process <-  fit(conv_model,
x = train_images, y = train_labels,
epochs = 10,
validation_data = list(test_images, test_labels),
verbose = 2)
#knitr::opts_chunk$set(echo = TRUE)
library(data.table)
library(cluster)
library(dbscan)
library(proxy)
library(mlbench)
library(ggplot2)
library(ggfortify)
library(gridExtra)
library(tidyverse)
library(dataPreparation)
library(stringr)
library(AMORE)
library(mlr3)
library(mlr3learners)
library(mlr3tuning)
datatrain <- read.csv2("FitFood_competition_data_training.csv", sep = ";")
datatest <- read.csv2("FitFood_competition_data_test.csv", sep = ";")
colnames(datatrain)
datatrain[,"will_it_sell"]
sum(sapply(datatest[,"product_id_unified"], function(x) sum(is.na(x))))
dim(datatrain)
unique(datatrain[,"cooking_mv"])
unique(datatrain[,"cooking_ov"])
unique(datatrain[,"partner_product"])
unique(datatrain[,"vat"])
s=0
for(i in 1:5360496){
if(datatrain[i,2]  == "5ba0ea5701f82e03b4131ace"){s=s+1}
}
s
#pos_id sie powtarzaja
sum(datatrain[,"pos_id"]=="5b1bb68ecaef965005d0e938")
#375 lodowek
#298 firm
#14 kategorii
#137 dań
#118 product_id_unified
#11770 wierszy z pos_id=5cd2c7b75314a576b410fb6f
#4426 pos_id=5b291c490663ab48e334abce
#23543 pos_id=5b1bb68ecaef965005d0e938
sapply(datatrain, function(X) any(is.na(X)))
#78 kolumn ma NA
w <- complete.cases(datatrain)
sum(datatrain[,1]=="0")
sum(w=="TRUE")
dim(datatrain[w,])
noNAtrain <- datatrain[rowSums(is.na(datatrain)) == 0, ]
set.seed(1234567)
training_idx <- sample(1:nrow(datatrain[rowSums(is.na(datatrain)) == 0, ]), round((nrow(datatrain[rowSums(is.na(datatrain)) == 0, ])/1000)))
training_set <- noNAtrain[training_idx, ]
#wyrzucamy cooking_mv bo Krzyś Pisiewicz powiedział, że ma wypierdalać i Andrzej Janusz powiedział, że ma racje
training_set <- training_set[,-13]
spr <- c(3,5:8,10:31)
train_spr <- training_set[,spr]
str(train_spr)
train_spr <- train_spr[order(train_spr$product_id_unified),]
sum(train_spr[1,]==train_spr[2,])
class <- lapply(training_set, class)
numer <- (class=="integer" | class=="numeric")
numer1 <-  (class == "factor" )
lista <- c("pos_id","company_id",)
## Zakładam bez sprawdzenia, że kolumna product_id_unified zawiera to samo co kolumny 3,5:8,10:31, więc się ich pozbywam
datatest1 <- datatest[,-c(5:8,10:32)]
set.seed(1234567)
training_idx1 <- sample(1:nrow(datatrain[rowSums(is.na(datatrain)) == 0, ]), round((nrow(datatrain[rowSums(is.na(datatrain)) == 0, ])/1000)))
View(datatest1)
View(datatest1)
set1 <- noNAtrain[training_idx1, ]
#wyrzucamy cooking_mv bo Krzyś Pisiewicz powiedział, że ma wypierdalać i Andrzej Janusz powiedział, że ma racje
set1 <- set1[,-c(5:8,10:32)]
str(set1)
set1[,1] <- as.factor(set1[,1])
task1 <- TaskClassif$new(id = "train1", backend = set1,
target = "will_it_sell")
train_set1 <- sort(sample(nrow(set1), floor(nrow(set1)*0.7)))
test_set1 <- setdiff(seq_len(task1$nrow), train_set1)
learner <- lrn("classif.rpart")
learner$predict_type = "prob"
measure <- msr("classif.auc")
learner$train(task1, row_ids = train_set1)
prediction <- learner$predict(task1, row_ids = test_set1)
odp<-prediction$prob[,2]
write(odp, file="odp.txt", sep = "\n")
prediction$score(measure)
View(noNAtrain)
View(datatrain)
View(datatest)
save.image("Z:/R/Systemy_Decyzyjne/Projekt/env_18_01.RData")
sum(is.na(datatrain[,2]))
for(i in 2:10){
sum(is.na(datatrain[,i]))
}
for(i in 2:10){
sum(is.na(datatrain[,i]))
}
for(i in 1:70){
print(sum(is.na(datatrain_only_id_unified[,i])))
}
datatrain_only_id_unified <- datatrain[,-c(5:8,10:32)]
length(unique(datatrain[,"product_id_unified"]))
str(datatrain_only_id_unified)
datatrain_only_id_unified[,1] <- as.factor(datatrain_only_id_unified[,1])
datatrain_only_id_unified[,3] <- as.factor(datatrain_only_id_unified[,3])
class <- lapply(datatrain_only_id_unified, class)
numer <- (class=="integer" | class=="numeric")
numer1 <-  (class == "factor" )
for(i in 9:70)
{
if(numer1[i]==T){
datatrain_only_id_unified[,i] <- as.numeric(datatrain_only_id_unified[,i])
}
}
#średnie
avg_col <- colMeans(datatrain_only_id_unified[,8:70],na.rm=TRUE)
for(col in 8:70){
datatrain_only_id_unified[,col][((is.na(datatrain_only_id_unified[,col])))] <- colMeans(datatrain_only_id_unified[,col],na.rm=TRUE)
}
for(col in 8:70){
datatrain_only_id_unified[,col][((is.na(datatrain_only_id_unified[,col])))] <- mean(datatrain_only_id_unified[,col],na.rm=TRUE)
}
#średnie
avg_col <- colMeans(datatrain_only_id_unified[,8:70],na.rm=TRUE)
remove(datatrain)
remove(noNAtrain)
remove(class,datatest,training_set,train_spr)
remove(set1)
remove(w)
#knitr::opts_chunk$set(echo = TRUE)
library(data.table)
library(proxy)
library(mlbench)
library(ggplot2)
library(ggfortify)
library(gridExtra)
library(tidyverse)
library(dataPreparation)
library(stringr)
library(AMORE)
library(mlr3)
library(mlr3learners)
library(mlr3tuning)
library(mlr3pipelines)
library(mlr3fselect)
library(paradox)
library(Matrix)
library(Seurat)
library(mltools)
library(xgboost)
atatrainsparse <- datatrain_only_id_unified
datatestsparse <- datatest1
datatrainsparse[,1] <- as.numeric(datatrainsparse[,1]) -1
datatrainsparse <- datatrain_only_id_unified
datatestsparse <- datatest1
datatrainsparse[,1] <- as.numeric(datatrainsparse[,1]) -1
datatestsparse[,1] <- as.numeric(datatestsparse[,1]) -1
duzyzbior_sparse <- rbind(datatrainsparse, datatestsparse)
remove(atatrainsparse)
duzyzbior_sparse <- sparsify(as.data.table(rbind(datatrainsparse, datatestsparse)))
duzyzbior_sparse <- sparsify(as.data.table(rbind(datatrainsparse, datatestsparse)))
?sample
dane<- read.csv2("plopln6m_q.csv", dec = ".")
source('Z:/R/Systemy_Decyzyjne/finanse.R', echo=TRUE)
source('Z:/R/Systemy_Decyzyjne/finanse.R', echo=TRUE)
source('Z:/R/Systemy_Decyzyjne/finanse.R', echo=TRUE)
source('Z:/R/Systemy_Decyzyjne/finanse.R', echo=TRUE)
install.packages("shiny")
library(shiny)
source('Z:/Kuba/Studia/uw/MatUbezp/obliczenia.R', echo=TRUE)
source('Z:/Kuba/Studia/uw/MatUbezp/obliczenia.R', echo=TRUE)
View(uk18)
View(uk18)
dim(uk18)
getwd()
source('Z:/Kuba/Studia/uw/MatUbezp/obliczenia.R', echo=TRUE)
source('Z:/Kuba/Studia/uw/MatUbezp/obliczenia.R', echo=TRUE)
View(uk18)
uk18 <- uk18[,-1]
type(uk18)
install.packages('rsconnect')
rsconnect::setAccountInfo(name='jbandurski',
token='AACA63C47E637103432BEF23C89D0EFC',
secret='n+CNMjwBGLcDekI1s4FFkyTQ1SYSpkYOuXw0Lg8v')
install.packages("shinythemes")
shiny::runApp('Z:/Kuba/Studia/uw/4_PSI/zaj_rshiny_5')
runApp('Z:/Kuba/Studia/uw/4_PSI/zaj_rshiny_5')
runApp('Z:/Kuba/Studia/uw/4_PSI/zaj_rshiny_5')
rsconnect::deployApp('Z:/Kuba/Studia/uw/4_PSI/zaj_rshiny_5')
c <- read.csv('count4.csv')
for(i in 1:10)
{
browseURL(as.character(<nazwa.wektora>[i,1]))
}
for(i in 1:10)
{
browseURL(as.character(c[i,1]))
}
for(i in 100:110)
{
browseURL(as.character(c[i,1]))
}
for(i in 3100:3110)
{
browseURL(as.character(c[i,1]))
}
for(i in 3111:3120)
{
browseURL(as.character(c[i,1]))
}
for(i in 3121:3130)
{
browseURL(as.character(c[i,1]))
}
for(i in 3131:3140)
{
browseURL(as.character(c[i,1]))
}
for(i in 3141:3160)
{
browseURL(as.character(c[i,1]))
}
for(i in 3161:3170)
{
browseURL(as.character(c[i,1]))
}
for(i in 3171:3180)
{
browseURL(as.character(c[i,1]))
}
for(i in 3181:3190)
{
browseURL(as.character(c[i,1]))
}
for(i in 3191:3200)
{
browseURL(as.character(c[i,1]))
}
for(i in 3201:3210)
{
browseURL(as.character(c[i,1]))
}
for(i in 3211:3220)
{
browseURL(as.character(c[i,1]))
}
for(i in 3221:3230)
{
browseURL(as.character(c[i,1]))
}
for(i in 3231:3240)
{
browseURL(as.character(c[i,1]))
}
for(i in 3241:3250)
{
browseURL(as.character(c[i,1]))
}
shiny::runApp('Z:/Kuba/Studia/uw/4_PSI/_projekty/Rshiny')
pluton
library(shiny)
#pakiety do systemow decyzyjnych
library(mlr3viz)
library(mlr3cluster)
library(mlr3)
#pakiety do wykresow
library(GGally)
library(ggplot2)
#pakiet do wygladu aplikacji
library(bslib)
runApp('Z:/Kuba/Studia/uw/4_PSI/_projekty/Rshiny')
?data
library(shiny)
#pakiety do systemow decyzyjnych
library(mlr3viz)
library(mlr3cluster)
library(mlr3)
#pakiety do wykresow
library(GGally)
library(ggplot2)
#pakiet do wygladu aplikacji
library(bslib)
#data
data(pluton)
runApp('Z:/Kuba/Studia/uw/4_PSI/_projekty/Rshiny')
knitr::opts_chunk$set(echo = TRUE)
library(data.table)
library(cluster)
library(dbscan)
library(proxy)
library(mlbench)
library(ggplot2)
library(ggfortify)
library(gridExtra)
library(mlr3viz)
library(mlr3cluster)
library(mlr3)
library(GGally)
library(ggplot2)
data(pluton)
pluton <- as.data.frame(scale(pluton,T,T))
saveRDS(pluton, file = "pluton.rds")
runApp('Z:/Kuba/Studia/uw/4_PSI/_projekty/Rshiny')
runApp('Z:/Kuba/Studia/uw/4_PSI/_projekty/Rshiny')
runApp('Z:/Kuba/Studia/uw/4_PSI/_projekty/Rshiny')
runApp('Z:/Kuba/Studia/uw/4_PSI/_projekty/Rshiny')
rsconnect::setAccountInfo(name='jakubbandurski',
token='974B9AD9338DC2E5338C59BF7CB64A85',
secret='<SECRET>')
rsconnect::setAccountInfo(name='jakubbandurski',
token='974B9AD9338DC2E5338C59BF7CB64A85',
secret='Ngo4BoAkdqpwxYyEAOaWuaPLUA0unvOz8P08y23Q')
library(rsconnect)
rsconnect::deployApp('Z:\Kuba\Studia\uw\4_PSI\_projekty\Rshiny')
library(rsconnect)
rsconnect::deployApp('Z:/Kuba/Studia/uw/4_PSI/_projekty/Rshiny')
accounts(server = NULL)
rsconnect::deployApp('Z:/Kuba/Studia/uw/4_PSI/_projekty/Rshiny',acount="jakubbandurski")
rsconnect::deployApp('Z:/Kuba/Studia/uw/4_PSI/_projekty/Rshiny',account="jakubbandurski")
runApp('Z:/Kuba/Studia/uw/4_PSI/_projekty/Rshiny')
runApp('Z:/Kuba/Studia/uw/4_PSI/_projekty/Rshiny')
install.packages("ggfortify")
install.packages("ggfortify")
shiny::runApp('Z:/Kuba/Studia/uw/4_PSI/_projekty/Rshiny')
runApp('Z:/Kuba/Studia/uw/4_PSI/_projekty/Rshiny')
library(ggfortify)
ibrary(ggfortify)
library(ggfortify)
library(ggplot2)
runApp('Z:/Kuba/Studia/uw/4_PSI/_projekty/Rshiny')
shiny::runApp('Z:/Kuba/Studia/uw/4_PSI/_projekty/Rshiny')
rsconnect::deployApp('Z:/Kuba/Studia/uw/4_PSI/_projekty/Rshiny',account="jakubbandurski")
install.packages(c("DT", "RPostgreSQL", "shinydashboard", "shinyjs", "sodium"))
source('Z:/Kuba/Studia/uw/4_PSI/_projekty/wercia/app.R')
source('Z:/Kuba/Studia/uw/4_PSI/_projekty/wercia/app.R')
source('Z:/Kuba/Studia/uw/4_PSI/_projekty/wercia/app.R')
shiny::runApp('Z:/Kuba/Studia/uw/4_PSI/_projekty/Rshiny')
runApp('Z:/Kuba/Studia/uw/4_PSI/_projekty/Rshiny')
qf(0.95,3,996)
qf(0.99,3,996)
stata <- data.frame(c(112,115,129,103,117,115,124,113,106,114,136,127))
colnames(stata)<-"x"
View(stata)
View(stata)
stata$x<-c(240, 230, 190, 160, 180, 210, 195, 180, 245)
stata$x<-[]
stata$x<-()
stata <- data.frame(c(240, 230, 190, 160, 180, 210, 195, 180, 245))
colnames(stata)<-"x"
stata$x<-c(240, 230, 190, 160, 180, 210, 195, 180, 245)
var(stata$x)
mean(stat$x)
mean(stata$x)
(55-140*50/270)^2/(140*50/270)+(80-140*140/270)^2/(140*140/270)+(60-140*80/270)^2/(140*80/270)+
(10-90*50/270)^2/(90*50/270)+(60-90*140/270)^2/(90*140/270)+(20-90*80/270)^2/(90*80/270)
knitr::opts_chunk$set(echo = TRUE)
tapply(iris$Sepal.Length, iris$Species, mean)
setwd("Z:\Kuba\Studia\uw\licencjatWNE\data\F-F_International_Countries.zip")
setwd("Z:/Kuba/Studia/uw/licencjatWNE/data/F-F_International_Countries.zip")
setwd("Z:/Kuba/Studia/uw/licencjatWNE/data")
readLines("UK.Dat", n=10)
